<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SEVA: Leveraging Sketches to Evaluate Alignment between Human and Machine Visual Abstraction">
  <!-- <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SEVA: Leveraging Sketches to Evaluate Alignment between Human and Machine Visual Abstraction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
      integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/sketch-gallery.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://kushinm.com/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SEVA: Leveraging Sketches to Evaluate Alignment between Human and Machine Visual Abstraction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kushinm.com/">Kushin Mukherjee</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://hollyhuey.github.io/">Holly Huey</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://xuanchenlu.com">Xuanchen Lu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yael-vinker.github.io/website/">Yael Vinker</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/raguinak">Rio Aguina-Kang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://faculty.runi.ac.il/arik/site/index.asp">Ariel Shamir</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://cogtoolslab.github.io/">Judith E Fan</a><sup>2,5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Wisconsin, Madison</span>
            <span class="author-block"><sup>2</sup>University of California, San Diego</span>
            <span class="author-block"><sup>3</sup>Tel-Aviv University</span>
            <span class="author-block"><sup>4</sup>Reichman University</span>
            <span class="author-block"><sup>5</sup>Stanford University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->

              <span class="link-block">
                <a href="https://arxiv.org/abs/xxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=PH8ZZud73Fw"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/cogtoolslab/visual_abstractions_benchmarking_public2023/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/cogtoolslab/visual_abstractions_benchmarking_public2023/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/seva_teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        We introduce SEVA, a new benchmark dataset containing 90K human-generated sketches that
        systematically vary in their level of detail, and evaluate a suite of state-of-the-art vision
        models in their ability to understand them.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Sketching is a powerful tool for creating abstract images that are sparse but meaningful. Sketch understanding poses
            fundamental challenges for general-purpose vision algorithms because it requires robustness to the sparsity of sketches
            relative to natural visual inputs and because it demands tolerance for semantic ambiguity, as sketches can reliably
            evoke multiple meanings. While current vision algorithms have achieved high performance on a variety of visual tasks, it
            remains unclear to what extent they understand sketches in a human-like way.
          </p>
          <p>
            Here we introduce <i>SEVA</i>, a new benchmark dataset containing 90K human-generated sketches of 128 object concepts produced
            under different time constraints, and thus systematically varying in sparsity. We evaluated a suite of state-of-the-art
            vision algorithms on their ability to correctly identify the target concept depicted in these sketches and to generate
            responses that are strongly aligned with human response patterns on the same sketch recognition task. We found that
            vision algorithms that better predicted human sketch recognition performance also better approximated human uncertainty
            about sketch meaning, but there remains a sizable gap between model and human response patterns.
          </p>
          <p>
            To explore the potential of models that emulate human visual abstraction in generative tasks, we conducted further
            evaluations of a recently developed sketch generation algorithm <a href="https://clipasso.github.io/clipasso/">(Vinker et al., 2022)</a> capable of generating sketches
            that vary in sparsity. We hope that public release of this dataset and evaluation protocol will catalyze progress
            towards algorithms with enhanced capacities for human-like visual abstraction.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/PH8ZZud73Fw"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Dataset. -->
      <div class="column is-full-width">
        <div class="content">
          <h2 class="title is-3">Visual abstraction as a key target for AI systems</h2>

          <div class="has-text-justified">
            <p>
              Sketching is a powerful tool for understanding human visual abstraction—how we distill semantically relevant information
              from our experiences of a visually complex world. In fact, it is one of the most prolific and enduring visualization
              techniques in human history for conveying our ideas in visual form. (As old as 40,000-60,000 years and found in most
              cultures!) Perhaps this is because, even without specialized training, most people including kids can make and interpret
              sketches.
            </p>
          </div>
          <div class="content" style="margin-left: auto; margin-right: auto; width: 50%;">
            <img src="./static/images/Picasso_The_Bull.jpg">
          </div>
          <div class="has-text-justified">
            <p>
              Take, for example, Picasso’s famous sketch series “The Bull” (1945). Despite some sketches being extremely sparse in
              details (right side) , each one is unmistakably a bull. What makes them so easily recognizable to us?
              Despite the fact that this ability to recognize visual concepts across levels of abstraction comes to us with such ease,
              it remains unclear to what extent current state-of-the-art vision algorithms are sensitive to visual abstraction in
              human-like ways.
            </p>
            <p>
              With a goal towards developing robust and generalized AI systems that can recognize sketches, diagrams, and glyphs in
              human-like ways, here we tackle two critical challenges:
            </p>

          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Dataset. -->
      <div class="column is-full-width">
        <div class="content">
          <h2 class="title is-3">SEVA: A novel Sketch benchmark for evaluating visual abstraction in humans and machines</h2>

          <div class="has-text-justified">
            <p>
              First, we needed to benchmark how humans generate sketches at varying levels of abstraction and across a diverse set of
              visual object concepts. To do this, we created the SEVA dataset! SEVA is a large-scale dataset of over 90,000 sketches
              that span 2,048 object instances belonging to 128 object categories. Because we wanted to test machine recognition of
              human visual abstraction, ~85,000 of these sketches were generated by people. But we also were also curious about
              machine recognition of machine visual abstraction and so ~5,000 sketches were generated by CLIPasso (Vinker et al.,
              2022).
              </p>

              <p>

              To vary the abstraction level of the sketches, we asked people (N=5,563 participants) to produce their drawings under
              different timing constraints: 4 seconds, 8 seconds, 16 seconds, and 32 seconds. We asked CLIPasso to generate its
              sketches under different stroke constraints: 4 strokes, 8 strokes, 16 strokes, and 32 strokes.

              </p>

              <p>
              Our work leverages a comprehensive set of object categories from the <a href="https://things-initiative.org/">THINGS</a>
              global initiative, an international research effort by multiple labs to develop large-scale datasets about the same but
              diverse range of object categories.

            </p>
          </div>
          <div class="content">
            <img src="./static/images/SEVA_dataset.png">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Dataset. -->
      <div class="column is-full-width">
        <div class="content">
          <h2 class="title is-3">Beyond accuracy: Evaluating alignment between human and machine visual abstraction</h2>
          <!-- <div class="content">
            <img src="./static/images/diagram_srgb.png">
          </div> -->
          <div class="row">
          <div class="col-lg-6 col-md-6 col-sm-12 col-xs-12 has-text-justified" style="min-width: 70%;">
            <p>
              Second, we wanted to evaluate how well state-of-the-art vision algorithms could recognize (i.e., correctly classify the
              target object category) the sketches, relative to human recognition performance and also how well the patterns of
              responses from AI models aligned with human patterns. To do this, we evaluated a diverse suite of 17 state-of-the-art
              vision models to evaluate human-model alignment. Building on prior research, we evaluated the models based on:
            </p>

            <p style="margin-left: 25px;">
              (1) <b>Top-1 classification accuracy</b>, reflecting raw sketch recognition performance.
            </p>

            <p>
              However, since our goal was to go beyond simple accuracy-based metrics, we also benchmarked models on two additional
              metrics:
            </p>

            <p style="margin-left: 25px;">
              (2) <b>Shannon entropy of the response distribution</b>, reflecting the degree of uncertainty about the target object category;
              and
            </p>

            <p style="margin-left: 25px;">
              (3) <b>Semantic neighbor preference</b>, reflecting the degree to which models and humans generated off-target responses that
              were semantically related to the target object category.
            </p>

            <p>
              By comparing the patterns of these metrics between models and humans, we empirically evaluated to what extent the
              machines' comprehension of visual abstraction differs from that of humans.


            </p>

          </div>
          <div class="column" style="width: 100%; padding-top: 0px;">
            <img src="./static/images/evaluation.png">
          </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Dataset. -->
      <div class="column is-full-width">
        <div class="content">
          <h2 class="title is-3">Measuring human and machine sketch understanding at multiple levels of abstraction</h2>
          <!-- <div class="content">
            <img src="./static/images/diagram_srgb.png">
          </div> -->
          <div class="has-text-justified">
            <p>
              We observe certain similarities in the response patterns in human and model evaluations, but a large gap remains
              between their sketch understandings. Our findings reveal that sparser sketches exhibit higher levels of semantic
              ambiguity for both models and humans. In contrast, more detailed sketches are associated with higher top-1
              classification performance, a tighter distribution of responses (lower entropy), and greater semantic neighbor
              preference.
            </p>
          </div>
          <div class="content" style="margin-left: auto; margin-right: auto; width: 90%;">
            <img src="./static/images/time_constraints.png">
          </div>

          <div class="has-text-justified">
            <p>

              Among all high-performing models, there are reliable differences between their response patterns under the three
              metrics, displaying systematic disparity in how they extract semantic information from sketches. For the time being,
              though, the state-of-the-art computer vision models exhibit a limited degree of alignment with humans at sketch
              understanding. A significant gap persists between the most closely aligned models and a baseline representing
              human-human consistency across all metrics.

            </p>
          </div>
          <div class="content" style="margin-left: auto; margin-right: auto; width: 100%;">
            <img src="./static/images/alignment.png">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Dataset. -->
      <div class="column is-full-width">
        <div class="content">
          <h2 class="title is-3">Do machine-generated sketches elicit similar responses as human sketches?</h2>
          <!-- <div class="content">
            <img src="./static/images/diagram_srgb.png">
          </div> -->
          <div class="has-text-justified">
            <p>
              While sketch understanding is a critical aspect of visual abstraction, the ability to produce sketches spanning
              different levels of abstraction is no less important. In our last experiment, we evaluated Clipasso, a CLIP-based vision
              model that was among the most performant and best aligned to human sketch understanding. Clipasso represents each sketch
              as a set of Bézier curves, and optimizes their parameters with respect to a CLIP-based perceptual loss. The model
              modulates the degree of abstraction by varying the number of strokes used. Our analysis reveals that human and Clipasso
              sketches are least divergent in their perceived meaning when they feature greater detail and are less abstract. In
              contrast, they exhibit more significant disparities when they have a larger degree of abstraction. This observation
              underscores the significant contrast between how CLIPasso and human participants attempted to preserve sketch meaning
              under more severe production constraints.

            </p>
          </div>
          <div class="content" style="margin-left: auto; margin-right: auto; width: 76%;">
            <img src="./static/images/human_CLIPasso.png">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Sketch Gallery</h2>
        <div class="content" id="sketch-gallery">
            <!-- <div id="sketch-gallery"></div> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{mukherjee2023SEVA,
  author    = {Mukherjee, Kushin and Huey, Holly and Lu, Xuanchen and Vinker, Yael and Aguina-Kang, Rio and Shamir, Ariel and Fan, Judith},
  title     = {SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2023}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/files/xxx.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/cogtoolslab/photo-sketch-correspondence" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align:center;">
            Thanks to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> for the website template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
